\renewcommand*{\afterpartskip}{
\vfil
\begin{epigraphs}
\qitem{\itshape
“Jo, hvis dét skal kaldes Fakta, saa benægter a Fakta!”
}{folketingsmedlem Søren Kjær, i debat med Carl Steen Andersen Bille}
\qitem{\itshape
  Politik skal ikke videnskabeliggøres. Der findes ikke noget facit i politik – kun følelser og holdninger. Begreber som sandt og falsk eller godt og ondt har ganske enkelt ikke hjemme i det politiske rum. 
}{Peter Skaarup, i et ugebrev for Dansk Folkeparti, \citeyear{skaarupPolitikErForst2017}}
\end{epigraphs}}

\part{Analyse}\label{part:analysis}

\chapter{At beskrive den sociale virkelighed}


Denne opgave tager udgangspunkt i, at der antages at være en intersubjektiv social virkelighed, og det er muligt at undersøge denne.
Jeg læner mig videre op af et socialkonstruktionistisk verdensbillede; hvor denne sociale verden er i kontinuerlig tilblivelse i en kollektiv samskabelsesproces \todo{kilde socialkonstruktionisme}.
Ikke dermed sagt, at der altid er enighed omkring konturene af denne samskabte sociale virkelighed — heller ikke i politik.
Dette understreges, måske lidt kækt, af Søren Kjær på titelbladet for denne del af specialet\todo{kontekst - man skal argumentere for sine udtalelser; og underbygge sine argumenter}.
Der er et spil der skal spilles; og præmien er en vis form for  “trappen” fra del III; og undervejs belyse mine undersøgelsesspørgsmål efterhånden som der dukker noget op; så at sige.
Jeg vil også illustrere de særegenheder jeg måtte finde i mit datasæt undervejs; 

\chapter{Indskrænkning af dokumenter til undersøgelse}

Jeg begynder med, at udarbejde et tf-idf objekt for hver analyseperiode.
En analyseperiode udgør således et korpus, hvor teksten er blevet forhåndsbehandlet og klargjort.
For at øge analyseværdien har jeg udeladt ord der falder under gennemsnitsværdien for \textit{tf-idf} for dermed at sortere de aller mes hyppigt forekommende ord fra.
For at undgå, at de aller mest sjældne ord forvrænger analysen, udelukker jeg herefter ord der har en \textit{tf-idf} i de yderste to promille.

Derefter foretager jeg en sammenligning af forskellige modeller for beregning af det optimale emner for videre analyse.

\begin{figure}
\input{../fig/models_edu_test_5to125by10.tex}
\caption{Beregning af optimalt antal emner for videre analyse; bigrams og ingen stopord.}
\label{fig:modelsFull}
\end{figure}

Figur ~\ref{fig:modelsFull} (side ~\pageref{fig:modelsFull}) viser en sammenligning af 4 forskellige modeller for udvælgelse af et "optimalt" antal emner for en LDA-baseret topic model.
Ved at se på kurvene fra disse sammenligninger, konvergerer kurvene omkring de 30-50 emner, inden de bevæger sig opad igen\footnote{Med undtagelse af \autocite{deveaudAccurateEffectiveLatent2014}; der ikke ser ud til at være en særlig hjælpsom algoritme i denne sammenhæng. Muligvis en konsekvens af mit noget specielle datasæt.}.
Jeg går videre med 45 emner, da det er her; der er størst enighed for hver kurve.

En LDA beregning giver ikke navngivne emner, som sådan.
I kraft af at være en ikke-superviseret algoritme, bliver dokumenterne fordelt ud over det anviste antal emner, i det omfang (algoritmen mener) de ligner hinanden.
vorvidt der er grupperinger der giver mening for mennesker i en social kontekst, og hvilken mening de eventuelt skulle have, er op til forskeren at vurdere.
Som eksempel præsenteres emnerne genereret for perioden 1990 til 2000; repræsenteret af de 15 hyppigste termer pr emne, rangeret efter prævalens.

\begin{figure}
\begin{adjustwidth}{-1in}{-1in}
  \input{../fig/terms_edu_test.tex}
\end{adjustwidth}
\caption{Oversigt over udvalgte emner for perioden 1990-2000, med tilhørende begreber.}
\label{fig:termsFull}
\end{figure}


Der er en gruppering (emne ??) der ser ud til at omhandle uddannelse.
Det viser sig, at de manglende stopord gav mange emner omhandlende administrivia. For at se om jeg kan øge informationstætheden forsøger jeg en ny runde; denne gang med en række generelle plus nogle domænespecifikke stopord\footnote{for den konkrete stopordsliste se \texttt{lib/stopwords.txt} i specialets GitHub-repositorie.}.

Dette giver følgende spredning:

\begin{figure}[H]
\centering
\input{../fig/models.tex}
\caption{Differentiering mellem emner for forskellige antal emner; bigrams og stopord. }
\end{figure}

Her kan man se, at det “optimale” antal emner for denne behandling af teksten ligger mellem ???.

Ved udtræk af modellen for ??? emner, får jeg emner repræsenteret af følgende termer:

\chapter{Analyse af de udvalgte dokumenter}

Jeg har nu en samling dokumenter, der ser ud til, at kunne omhandle uddannelse.
Disse trækkes ud til videre analyse.
Først vil jeg se, hvilke termer der er gennemgående i dokumenterne, med en ordoptælling.
Derefter vil jeg undersøge, hvorvidt der er emner at trække ud indenfor disse dokumenter.
Jeg vil derefter køre en analyse, der har til sigte at afdække partipolitiske positioner over dette subset.

\section{Gennemgående termer for hver periode}

\begin{figure}
  \caption{De 20 mest omtalte termer for de udvalgte dokumenter, fordelt over de definerede analyseperioder.} 
\end{figure}

\subsection{Hvilke begreber er kendetegnende for de politiske fløje}
\todo{hvordan dele op i fløje}

\section{Emner indenfor uddannelse}

\section{Partipolitiske positioner}

Ordoptellingen ovenfor er en noget simplistisk tilgang til at afdække partipolitiske positioner.
Der er udarbejdet konkrete algoritmer for at undersøge dette.
Jeg vil benytte mig af den før omtalte \texttt{wordfish}-algoritme \autocite{slapinScalingModelEstimating2008}.

